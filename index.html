<html>
  <!-- * import party -->
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <title>Zelin (Friday) Gao</title>
    <link href="./css/style.css" rel="stylesheet" media="all" type="text/css"/>
    <link href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css" rel="stylesheet"/>
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¥³</text></svg>">
    <script type="text/javascript" src="https://code.jquery.com/jquery-2.2.0.min.js"></script>
    <script src="https://kit.fontawesome.com/57fb8d417e.js" crossorigin="anonymous"></script>
    <script type="text/javascript">
      window.onload = choosePic;
      var myPix = new Array("./assets/self/gang.jpeg");
      function choosePic() {
        var randomNum = Math.floor(Math.random() * myPix.length);
        document.getElementById("myPicture").src = myPix[randomNum];
      }
      function lastUpdate() {
        var x = document.lastModified.substr(0, 10);
        document.getElementById("demo").innerHTML = x;
      }
    </script>
  </head>

  <body onload="lastUpdate()">
    <div class="content">
      <div id="container">
        <!-- * my personal link -->
        <table>
          <tbody>
            <tr>
              <!-- column one, is my self-portrait! -->
              <td>
                <img 
                id="portrait" 
                src="./assets/self/gang.jpeg" 
                style="float: left; margin-top: 60px; margin-left: 60px; margin-bottom: 10px; border-radius: 10%;"
                width="200px"/>
              </td>
              <!-- column two, is my info! -->
              <td>
                <div id="DocInfo">
                  <div id="intro">
                    <h1 style="margin-bottom: 10px;"> Zelin (Friday) Gao éƒœæ³½éœ– </h1>
                    <div style="margin-bottom: 10px;">
                      <a style="color: black" href="https://jameskuma.github.io/">
                        State: ðŸ˜Œ Start A New Journey!
                      </a>
                    </div>
                    <div style="margin-bottom: 10px;">
                      <a href="mailto: zelingao98@gmail.com">
                        <span class="fa fa-envelope" style="color: navy; font-size: 20px">
                          <span style="font-family: Garamond Bold">
                            zelingao98
                          </span>
                          <span class="fa fa-at" style="color: navy; font-size: 20px">
                          <span style="font-family: Garamond Bold">
                            gmail.com
                          </span>
                        </span>
                      </a>
                    </div>
                    <ul class="icon-list">
                      <!-- <a style="color: black" href="./assets/self/cv_en_gzl.pdf">
                        <span class="ai ai-cv fa-xl"></span>
                      </a> -->
                      <a style="color: black" href="https://scholar.google.com/citations?user=kvHYP9MAAAAJ&hl=zh-CN">
                        <span class="ai ai-google-scholar fa-xl"></span>
                      </a>
                      <a style="color: black" href="https://github.com/zelingao98">
                        <span class="fa-brands fa-github fa-xl"></span>
                      </a>
                      <a style="color: black" href="https://x.com/Friday_GaoZL">
                        <span class="fa-brands fa-twitter fa-xl"></span>
                      </a>
                      <!-- <a style="color: black" href="https://space.bilibili.com/17005936">
                        <span class="fa-brands fa-bilibili fa-xl"></span>
                      </a> -->
                    </ul>
                  </div>
                </div>
              </td>
            </tr>
          </tbody>
        </table>
        <!-- * my bio -->
        <table>
          <tr>
            <td>
              <h1>Biography</h1>
              <strong style="color: tomato;">
                I am actively looking for research collaboration. 
                Please feel free to contact me if you feel confused about the future or you are experiencing a hard time.
                I am always happy to help as I was also in that situation for the past few days.
              </strong>
              <hr>
              <!-- segment line, above shows important message -->
              <div style="color: black; margin-bottom: 10px; text-align: justify;">
                <!-- I was a research intern at <a style="font-weight: bold;" href="https://xiaolonw.github.io/">Xiaolong Wang's Gruop</a>. -->
                I got M. Eng (2021-2024) in <a style="font-weight: bold;" href="http://www.cse.zju.edu.cn/cseenglish/main.htm">CSE Department</a> at <a style="font-weight: bold;" href="https://www.zju.edu.cn/">Zhejiang University</a>, where I was advised by Prof. Yu Zhang in IAS Lab.
                I got my B. Eng (2017-2021) in <a style="font-weight: bold;" href="https://ciee.jlu.edu.cn/en/">EE Department</a> at <a style="font-weight: bold;" href="https://www.jlu.edu.cn/">Jilin University</a>, where I spent 4 wonderful years.
              </div>
              <div style="color: black; margin-bottom: 10px; text-align: justify;">
                I feel extremely fortunate to collaborate with 
                <a style="font-weight: bold;" href="https://jerryxu.net/">Dr. Jiarui Xu</a> and <a style="font-weight: bold;" href="https://xiaolonw.github.io/">Prof. Xiaolong Wang</a> at UCSD,
                <a style="font-weight: bold;" href="https://research.nvidia.com/person/shalini-de-mello">Dr. Shalini De Mello</a> and <a style="font-weight: bold;" href="https://swook.net/">Dr. Seonwook Park</a> at NVIDIA Research,
                <a style="font-weight: bold;" href="https://yutongbai.com/">Dr. Yutong Bai</a> and <a style="font-weight: bold;" href="https://people.eecs.berkeley.edu/~kanazawa/">Prof. Angjoo Kanazawa</a> at UCB,
                <a style="font-weight: bold;" href="https://weichnn.github.io/">Prof. Weichen Dai</a> at Hangzhou Dianzi University.
                They are the ones who introduce me into the world of research and teach me what it means to be a good and honest researcher. 
                They will always be my role models!
              </div>
              <div style="color: black; text-align: justify;">
                My research interest lies in computer vision, especially in fields of 3D/4D Reconstruction and Generation, as well as the fusion of multimodal data.
                I also find human body models such as SMPL and SMPL-X super awesome!
              </div>
              <!-- <div style="color: black; text-align: center;">
                <img src="./assets/self/research_direction.png" height="350px" width="750px">
              </div> -->
            </td>
          </tr>
        </table>
        <!-- * my recent news -->
        <h1>News</h1>
        <!-- <div style="height: 100px; overflow: auto"> -->
        <div>
          <div>
            [2025.05] I have made my final decision for the future.
          </div>
          <div>
            [2025.04] I successfully lose weight! More than 40æ–¤. Congratulations to myslef!
          </div>
          <div>
            [2024.12] Our paper is accepted by AAAI2025!
          </div>
          <div>
            [2024.04] GAP year starts! Hang on! I will get through these bad days.
          </div>
          <div>
            [2024.03] I defend my thesis and receive my master's degree!
          </div>
          <div>
            [2024.02] Our paper is accepted by CVPR2024 as Highlight!
          </div>
          <div>
            [2023.07] Our paper is accepted by ICCV2023!
          </div>
          <div>
            [2022.06] Our paper is accepted by IROS2022&RAL as Oral Presentation!
          </div>
        </div>

        <h1>Publications</h1>
        Representative works are <span style="background-color: lightgoldenrodyellow">highlighted</span> (# denotes equal contribution)

        <table class="pub_table" style="border-collapse:separate; border-spacing: 0px 5px;">
          <tr id="arxiv2025" class="focus">
            <td class="pub_td1">
              <video autoplay loop muted height="150px" width="180px">
                <source src="./assets/teasers/aha_cvpr2025.mp4" type=video/mp4>
              </video>
            </td>
            <td class="pub_td2">
              <div style="margin-top: 5px; margin-bottom: 5px">
                <strong>AHA: Expressive Animation of Humans Driven by Audio</strong>
              </div>
              <u>Zelin Gao</u>,
              <a href="https://jerryxu.net/">Jiarui Xu</a>,
              <a href="https://swook.net/">Seonwook Park</a>,
              <a href="https://luminohope.org/">Koki Nagano</a>,
              <a href="https://sifeiliu.net/">Sifei Liu</a>,
              <a href="https://xiaolonw.github.io/">Xialong Wang</a>,
              <a href="https://research.nvidia.com/person/shalini-de-mello">Shalini De Mello</a>
              <div style="margin-top: 5px; margin-bottom: 5px">
                Under Review
              </div>
              <div style="margin-top: 5px; margin-bottom: 5px">
                <a class="box" href="https://jameskuma.github.io/">
                  <i class="fas fa-home" aria-hidden="true">&nbsp;</i>
                  Home (coming soon)&nbsp;
                </a>
                <a class="box" href="https://jameskuma.github.io/">
                  <i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i>
                  Paper (coming soon)&nbsp;
                </a>
                <a class="box" href="https://jameskuma.github.io/">
                  <i class="fa fa-github" aria-hidden="true">&nbsp;</i>
                  Code (coming soon)&nbsp;
                </a>
              </div>
              <div style="text-align: justify;">
                We present AHA, a novel approach to driving 2D human portrait animation that generates upper body motion, hand gestures, lip movements, and facial expressions.
                AHA allows for a versatile solution capable of being driven by audio alone and/or video, where both inputs complement and augment each other. 
              </div>
            </td>
          </tr>

          <tr id="aaai2025wavpe">
            <td class="pub_td1">
              <img src="./assets/teasers/wavpe_aaai2025.png" height="150px" width="180px">
            </td>
            <td class="pub_td2">
              <div style="margin-top: 5px; margin-bottom: 5px">
                <strong>Adaptive Wavelet-Positional Encoding for High-Frequency Information Learning in INR</strong>
              </div>
              Hongxu Zhao,
              <u>Zelin Gao</u>,
              <a href="https://person.zju.edu.cn/en/zhangyu">Yu Zhang</a>
              <div style="margin-top: 5px; margin-bottom: 5px">
                AAAI 2025
              </div>
              <div style="margin-top: 5px; margin-bottom: 5px">
                <a class="box" href="https://jameskuma.github.io/">
                  <i class="fas fa-home" aria-hidden="true">&nbsp;</i>
                  Home (coming soon)&nbsp;
                </a>
                <a class="box" href="https://jameskuma.github.io/">
                  <i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i>
                  Paper (coming soon)&nbsp;
                </a>
              </div>
              <div style="text-align: justify;">
                The implicit representation is incomplete since different components of the signal align with different frequency bands and neural network is proved to inherently converge to low frequency.
                Inspired by Wavelet regression, we propose Adaptive Wavelet-Positional Encoding to represent content under different frequency distributions.
              </div>
            </td>
          </tr>

          <tr id="cvpr2024sap3d" class="focus">
            <td class="pub_td1">
              <img src="./assets/teasers/spa3d_cvpr2024.png" height="150px" width="180px">
            </td>
            <td class="pub_td2">
              <div style="margin-top: 5px; margin-bottom: 5px">
                <strong>SAP3D: The More You See in 2D, the More You Perceive in 3D</strong>
              </div>
              Xinyang Han#,
              <u>Zelin Gao#</u>,
              <a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a>,
              <a href="https://shubham-goel.github.io/">Shubham Goel</a>,
              <a href="https://yossigandelsman.github.io/">Yossi Gandelsman</a>
              <div style="margin-top: 5px; margin-bottom: 5px">
                CVPR 2024, <strong style="color: brown">Highlight</strong>
              </div>
              <div style="margin-top: 5px; margin-bottom: 5px">
                <a class="box" href="https://sap3d.github.io/">
                  <i class="fas fa-home" aria-hidden="true">&nbsp;</i>
                  Home&nbsp;
                </a>
                <a class="box" href="https://arxiv.org/abs/2404.03652">
                  <i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i>
                  Paper&nbsp;
                </a>
                <a class="box" href="https://github.com/sap3d/Sap3D">
                  <i class="fa fa-github" aria-hidden="true">&nbsp;</i>
                  Code
                </a>
              </div>
              <div style="text-align: justify;">
                Humans can infer 3D structure from 2D images of an object based on past experience and improve their 3D understanding as they see more images. 
                Inspired by this behavior, we introduce SAP3D, a system for 3D reconstruction and novel view synthesis from an arbitrary number of unposed images.
              </div>
            </td>
          </tr>

          <tr id="arxiv2024d4dream">
            <td class="pub_td1">
              <video autoplay loop muted height="150px" width="180px">
                <source src="./assets/teasers/d4dreamer_arxiv2024.mp4" type=video/mp4>
              </video>
            </td>
            <td class="pub_td2">
              <div style="margin-top: 5px; margin-bottom: 5px">
                <strong>D4-Dreamer: Text-to-Non-Rigid Scene Generation</strong>
              </div>
              <u>Zelin Gao</u>,
              <a href="https://yucornetto.github.io/">Qihang Yu</a>,
              <a href="https://yutongbai.com/">Yutong Bai</a>
              <div style="margin-top: 5px; margin-bottom: 5px">
                ARXIV 2024
              </div>
              <div style="margin-top: 5px; margin-bottom: 5px">
                <a class="box" href="https://jameskuma.github.io/">
                  <i class="fas fa-home" aria-hidden="true">&nbsp;</i>
                  Home (coming soon)&nbsp;
                </a>
                <a class="box" href="https://jameskuma.github.io/">
                  <i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i>
                  Paper (coming soon)&nbsp;
                </a>
                <a class="box" href="https://jameskuma.github.io/">
                  <i class="fa fa-github" aria-hidden="true">&nbsp;</i>
                  Code (coming soon)
                </a>
              </div>
              <div style="text-align: justify;">
                D4-Dreamer is a text-to-non-rigid scene generation framework. 
                We introduce two separate guidance for scene generation to optimize non-rigid radiance fields, where we drive fantasy content generation from scene content guidance (SDS-MV) and reasonable motion generation from scene motion guidance (VSD-T).
              </div>
            </td>
          </tr>

          <tr id="arxiv2023hgnerf">
            <td class="pub_td1">
              <img src="./assets/teasers/hgnerf_arxiv2023.png" height="110px" width="180px">
            </td>
            <td class="pub_td2">
              <div style="margin-top: 5px; margin-bottom: 5px">
                <strong>HG3-NeRF: Hierarchically Guided Neural Radiance Fields for Sparse View Inputs</strong>
              </div>
              <u>Zelin Gao</u>,
              <a href="https://weichnn.github.io/">Weichen Dai</a>,
              <a href="https://person.zju.edu.cn/en/zhangyu">Yu Zhang</a>
              <div style="margin-top: 5px; margin-bottom: 5px">
                ARXIV 2023
              </div>
              <div style="margin-top: 5px; margin-bottom: 5px">
                <a class="box" href="https://arxiv.org/abs/2401.11711">
                  <i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i>
                  Paper&nbsp;
                </a>
              </div>
              <div style="text-align: justify;">
                To represent the neural radiance fields from sparse view inputs,
                we propose hierarchical geometric guidance (HGG) to sample volume points with the depth prior 
                and 
                hierarchical semantic guidance (HSG) to supervise semantic consistency of the complex real-world scenarios using CLIP.
              </div>
            </td>
          </tr>

          <tr id="iccv2023apenerf" class="focus">
            <td class="pub_td1">
              <video autoplay loop muted height="120px" width="180px">
                <source src="./assets/teasers/apenerf_iccv2023.mp4" type=video/mp4>
              </video>
            </td>
            <td class="pub_td2">
              <div style="margin-top: 5px; margin-bottom: 5px">
                <strong>Adaptive Positional Encoding for Bundle-Adjusting Neural Radiance Fields</strong>
              </div>
              <u>Zelin Gao</u>,
              <a href="https://weichnn.github.io/">Weichen Dai</a>,
              <a href="https://person.zju.edu.cn/en/zhangyu">Yu Zhang</a>
              <div style="margin-top: 5px; margin-bottom: 5px">
                ICCV 2023
              </div>
              <div style="margin-top: 5px; margin-bottom: 5px">
                <a class="box" href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_Adaptive_Positional_Encoding_for_Bundle-Adjusting_Neural_Radiance_Fields_ICCV_2023_paper.pdf">
                  <i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i>
                  Paper&nbsp;
                </a>
              </div>
              <div style="text-align: justify;">
                Adaptive Positional Encoding is proposed to train neural radiance fields from unknwon camera poses (or even initrinics).
                The theoretical relationship between Positional Encoding and Fourier Series Regression is investigated to prove that learnable frequencies can improve both camera parameter estimation and NVS qualiy.
              </div>
            </td>
          </tr>

          <tr id="iros2022slam">
            <td class="pub_td1">
              <img src="./assets/teasers/tislam_iross2022.jpg" height="90px" width="180px">
            </td>
            <td class="pub_td2">
              <div style="margin-top: 5px; margin-bottom: 5px">
                <strong>Thermal-Inertial SLAM for the Environments with Challenging Illumination</strong>
              </div>
              Jiajun Jiang,
              <a href="https://weichnn.github.io/">Weichen Dai</a>,
              Xingxin Chen,
              <u>Zelin Gao</u>,
              <a href="https://person.zju.edu.cn/en/zhangyu">Yu Zhang</a>
              <div style="margin-top: 5px; margin-bottom: 5px">
                IROS2022&RAL, <strong style="color: brown">Oral Presentation</strong>
              </div>
              <div style="margin-top: 5px; margin-bottom: 5px">
                <a class="box" href="https://www.bilibili.com/video/BV1T94y1U7Ec/?vd_source=59aacd2e402d02cfc05c523d11873aa7">
                  <i class="fas fa-home" aria-hidden="true">&nbsp;</i>
                  Home&nbsp;
                </a>
                <a class="box" href="https://ieeexplore.ieee.org/document/9804793">
                  <i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i>
                  Paper&nbsp;
                </a>
              </div>
              <div style="text-align: justify;">
                Thermal images are used in this paper to realize a robust visual SLAM system in challenging environment. 
                The proposed method, a thermal-inertial SLAM system, represents several improvements, including SVD-based image processing and Thermal-RAFT tracking methods.
              </div>
            </td>
          </tr>

        </table>

        <!-- * update time -->
        <div align="center">
          <div>
            <script
              type="text/javascript"
              src="//rf.revolvermaps.com/0/0/6.js?i=5defqxh6kgm&amp;m=1&amp;c=ff0000&amp;cr1=ffffff&amp;f=ubuntu&amp;l=0&amp;s=300&amp;bv=100&amp;hi=20"
              async="async"
            ></script>
          </div>
          <strong>
            <p style="text-align: right; margin-bottom: 0px; font-size: smaller;">
              This page is from <a href="https://xiuyuliang.cn/">Prof. Yuliang Xiu</a>. His website looks really nice! Big thanks!
            </p>
          </strong>
        </div>
      </div>
    </div>
  </body>
</html>
